# DiffPriv

[**Comparative Metrics Framework.pdf**](https://github.com/MUNFAI15/DiffPriv/blob/master/comparative%20metrics%20framework.pdf) provides a comprehensive and intuitive framework to evaluate the performance of any synthetic dataset vis-Ã -vis its original dataset and other synthetic datasets. It captures information on the tradeoff between **general utility, specific utility** and **disclosure risk**. 

### Motivation

Whilst researching on the best data synthesis method, I struggled with finding suitable benchmarks to evaluate the quality of synthetic datasets. Taub et al.'s work on this is highly excellent, but was not easily reproducible. 

> I coded previously publicly unavailable functions on ROC and DCAP in the package **cmf**. 
>
> I made modifications to Taub's work when necessary, to ensure an intuitive, comprehensive and systematic evaluation of synthetic datasets. 

For code on the ROC and CAP functions as well as their usage examples, refer to the package [**cmf**](https://github.com/MUNFAI15/cmf).

Here is a high-level overview of the comparative metrics framework. 
![Screenshot](https://github.com/MUNFAI15/DiffPriv/blob/master/CMF%20Overview.png)

### Usage Examples 
Here, we synthesise the mtcars dataset with the package **synthpop**. Many of the metrics in this comparative metrics framework are built in **synthpop** functions; however, they can be applied to any synthetic dataset generated by other methods.  

```bash
library(cmf)
library(synthpop)
df <- mtcars
syn1 <- syn(df, seed = 1234)
synthpop_df <- syn1$syn
```
#### General Utility 

```bash
### ROC 
ROC_list(df, synthpop_df)
ROC_indiv(df, synthpop_df, "disp")
ROC_score(df, synthpop_df)
ROC_numeric(df, synthpop_df, "disp", y=2)
```
#### Specific Utility 

#### Disclosure Risk 
```bash
key_var <- c("cyl", "gear")
target_var <- c("wt", "carb")

### CAP 
CAP_original(df, key_var, target_var)
CAP_baseline(df, target_var)
CAP_synthetic(df, synthpop_df, key_var, target_var)
```


Author : Chan Mun Fai 
